{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8954a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from time import time, strftime, gmtime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from loss_functions import *\n",
    "from train_test import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb241e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(eta=0.0):\n",
    "    \n",
    "    # 100 features\n",
    "    # 10 classes\n",
    "    \n",
    "    X_train = np.load('data/Xtrain-eta{}-data.npy'.format(eta))\n",
    "    X_test  = np.load('data/Xtest-eta{}-data.npy'.format(eta))\n",
    "    y_train = np.load('data/ytrain-eta{}-label.npy'.format(eta))\n",
    "    y_test  = np.load('data/ytest-eta{}-label.npy'.format(eta))\n",
    "\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=float), torch.tensor(y_train, dtype=int))\n",
    "    test_data  = TensorDataset(torch.tensor(X_test, dtype=float), torch.tensor(y_test, dtype=int))\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size_train = 20\n",
    "    batch_size_test  = 20\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_data,  batch_size=batch_size_test,  shuffle=True)\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "        print(\"Shape of y: \", y.shape, y.dtype)\n",
    "        break\n",
    "        \n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b0ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=80, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=20, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model: feedforward network 100->80->40->20->10\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bb6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataloader, test_dataloader, model, loss_fn, optimizer, epochs):\n",
    "\n",
    "    train_loss0 = []\n",
    "    train_acc0  = []\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc  = []\n",
    "    test_loss  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    tr_ls, tr_ac = test(train_dataloader, model, loss_fn)\n",
    "    train_loss.append(tr_ls)\n",
    "    train_acc.append(tr_ac)\n",
    "\n",
    "    te_ls, te_ac = test(test_dataloader, model, loss_fn)\n",
    "    test_loss.append(te_ls)\n",
    "    test_acc.append(te_ac)\n",
    "\n",
    "    for t in tqdm(range(epochs)):\n",
    "        #print(f\"Epoch {t+1}\")\n",
    "\n",
    "        tr_ls0, tr_ac0 = train(train_dataloader, model, loss_fn, optimizer)\n",
    "        train_loss0.extend(tr_ls0)\n",
    "        train_acc0.extend(tr_ac0)  \n",
    "\n",
    "        tr_ls, tr_ac = test(train_dataloader, model, loss_fn)\n",
    "        train_loss.append(tr_ls)\n",
    "        train_acc.append(tr_ac)\n",
    "\n",
    "        te_ls, te_ac = test(test_dataloader, model, loss_fn)\n",
    "        test_loss.append(te_ls)\n",
    "        test_acc.append(te_ac)\n",
    "\n",
    "        #print(f\"Test accuracy: {(100*te_ac):>0.1f}%, Test loss: {te_ls:>8f}\")\n",
    "        \n",
    "    return train_loss0, train_acc0, train_loss, train_acc, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e982eaed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***eta=0.0***\n",
      "Shape of X [N, C, H, W]:  torch.Size([20, 100])\n",
      "Shape of y:  torch.Size([20]) torch.int64\n",
      "\n",
      "**Experiment: 1/5**\n",
      "MSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▌                                     | 3/20 [00:02<00:14,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 54\u001b[0m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     53\u001b[0m train_loss0, train_acc0, train_loss, train_acc, test_loss, test_acc\\\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m filename \u001b[38;5;241m=\u001b[39m folder\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/synthetic_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eta\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_trial\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(titles[i_loss],lr,eta,i_trial)\n\u001b[1;32m     57\u001b[0m np\u001b[38;5;241m.\u001b[39msave(filename, [train_loss0, train_acc0, train_loss, train_acc, test_loss, test_acc])\n",
      "Cell \u001b[0;32mIn [4], line 26\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m train_loss0\u001b[38;5;241m.\u001b[39mextend(tr_ls0)\n\u001b[1;32m     24\u001b[0m train_acc0\u001b[38;5;241m.\u001b[39mextend(tr_ac0)  \n\u001b[0;32m---> 26\u001b[0m tr_ls, tr_ac \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(tr_ls)\n\u001b[1;32m     28\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(tr_ac)\n",
      "File \u001b[0;32m~/Documentos/Mestrado/Pesquisa/Simulações/fisher-rao-loss-function/Synthetic data/../train_test.py:46\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m correct   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     47\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create folder\n",
    "folder = 'results-test'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "def my_MSE_loss(x,y):\n",
    "    return MSE_loss(x,y,10)\n",
    "\n",
    "def my_MAE_loss(x,y):\n",
    "    return MAE_loss(x,y,10)\n",
    "\n",
    "n_trials = 5\n",
    "epochs   = 20\n",
    "\n",
    "# Loss functions\n",
    "Loss_fn = [my_MSE_loss, my_MAE_loss, CE_loss, FR_loss, H_loss]\n",
    "titles  = ['MSE', 'MAE', 'Cross-entropy', 'Fisher-Rao', 'Hellinger']\n",
    "\n",
    "# Learning rates\n",
    "Lr_all   = [[0.4], [0.2], [0.04], [0.04], [0.04]]\n",
    "\n",
    "# Noise rates\n",
    "eta_all = [0.0]\n",
    "\n",
    "# For each eta\n",
    "for eta in eta_all:\n",
    "    \n",
    "    print('\\n***eta={}***'.format(eta))\n",
    "\n",
    "    # Load data\n",
    "    train_dataloader, test_dataloader = get_data(eta=eta)\n",
    "    \n",
    "    # For each experiment (trial)\n",
    "    #for i_trial in range(n_trials):\n",
    "    for i_trial in range(1,5):\n",
    "\n",
    "        tic = time()\n",
    "        print('\\n**Experiment: {}/{}**'.format(i_trial,n_trials))\n",
    "\n",
    "        # Choose loss function\n",
    "        for i_loss in range(len(Loss_fn)):\n",
    "\n",
    "            loss_fn = Loss_fn[i_loss]\n",
    "            print(titles[i_loss])\n",
    "\n",
    "            # Choose learning rate\n",
    "            for lr in Lr_all[i_loss]:\n",
    "\n",
    "                model = NeuralNetwork().to(device)\n",
    "\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "                train_loss0, train_acc0, train_loss, train_acc, test_loss, test_acc\\\n",
    "                        = run(train_dataloader, test_dataloader, model, loss_fn, optimizer, epochs)\n",
    "\n",
    "                filename = folder+'/synthetic_{}_lr{}_eta{}_trial{}'.format(titles[i_loss],lr,eta,i_trial)\n",
    "                np.save(filename, [train_loss0, train_acc0, train_loss, train_acc, test_loss, test_acc])\n",
    "\n",
    "        toc = time()\n",
    "    print('Elapsed time: ' + strftime(\"%H:%M:%S\", gmtime(toc-tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf8134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
